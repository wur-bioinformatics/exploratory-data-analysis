# Debugging and testing {#sec-debugging-and-testing}

```{r setup}
#| echo: FALSE
# We want errors to show up in this notebook
options(error = recover)
# this code is only for getting the data into the notebook during publishing
data(corre_continuous, package = "planteda")
data(grassland_traits_environment, package = "planteda")
```

Programming and scripting for data analysis is _not only_ about writing code that runs.
It is perhaps even more important to write code that produces correct and interpretable results.

Errors in code can become especially dangerous when they silently produce wrong results without throwing errors.
In this chapter we will go over different types of errors and warnings and how you can solve and prevent them.

:::{#wrn-analysis-gone-wrong .callout-warning appearance='simple'}
## Data analysis code gone wrong, famous examples
In the late 2000s, researchers claimed that gene-expression data from cancer cell lines could be used to predict which chemotherapy drugs would work for individual patients. 
These results were considered so promising that human clinical trials were started based on the predictions.
Later, independent scientists tried to reproduce the analysis and found that the problem was not the biological question or the statistical methodology, but simple bugs in the data-analysis code [@Baggerly2009].
Gene-expression tables had been misaligned with patient labels, some samples had been accidentally swapped, and categorical variables had been silently reordered by the software.
Because the analysis pipeline contained no checks that data and labels still matched, these errors went unnoticed.
The models therefore looked highly accurate, while in reality they were making predictions close to random guessing.
When the mistakes were corrected, the supposed drug-response signatures disappeared.
Several papers were withdrawn and the clinical trials were stopped.
This episode is now a classic warning that small programming errors can lead to large scientific and real-world consequences.
The take home message: careful debugging and reproducible workflows matter in data-driven biology.
:::


We will work on two complementary skill sets:

- Debugging and error handling: what to do when your code (or someone else's code!) does not work as intended.
- Testing and validating: how to formally verify your code works as intended

## Debugging and error handling

[Bugs](https://en.wikipedia.org/wiki/Software_bug) and errors go hand in hand: a __bug__ is an error or flaw in a program that causes it to behave differently from what the programmer intended. Bugs can cause code to crash with an error, produce warnings, or even run without complaint while giving incorrect results.

Bugs arise for many reasons: misunderstandings about how a function works, incorrect assumptions about the data, edge cases that were not considered, or simple typing mistakes. Importantly, a program can be syntactically correct (it runs) and still contain bugs if the logic is wrong.

__Debugging__ is the process of identifying, understanding, and fixing these bugs. Rather than trial-and-error, effective debugging relies on systematically inspecting code, checking assumptions, and narrowing down where the programâ€™s behavior diverges from expectations.

:::{.callout-note appearance='simple'}
### About the term "bug"
The word "bug" to describe a programming error is often linked to [Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper). In 1947, while working on the Harvard Mark II computer, her team found that a malfunction was caused by a real moth trapped in a relay (back then computers where large devices that filled entire rooms). The insect was taped into the logbook with the note "First actual case of bug being found".

Although the term bug was already used informally in engineering, this incident popularized it in computing. It nicely illustrates an important idea: bugs can arise from unexpected causes, and finding them requires careful investigation rather than guesswork.
:::

:::{#tip-how-to-debug .callout-tip appearance='simple'}
### How to debug
The main approach to debugging (potentially complex) code is to be systematic. The following steps help in identifying the underlying problem:

1. __Reproduce the issue__: make sure you consistently see the bug. Does it only occur on a specific dataset? Or under other specific conditions? These can provide hints.
2. __Isolate the problem__: (especially when working with larger functions or big datasets) Narrow down where the error _actually_ comes from. Adding print statements or commenting out specific sections can help. Switching to a small test dataset can be useful too.
3. __Understand/hypothesize__: once you figured out where the error occurs, check what you expected to happen and verify this is (or is not) happening.
4. __Fix and verify__: Implement the fix, then _test_ the code by reproducing the original conditions under which the error occurred.
:::

To better understand what can cause your code to misbehave, it is important to understand a little bit about different types of errors in R. The following section covers syntax errors, runtime errors, warnings, and logical errors.

### Syntax errors
These errors often pop up when you are in the process of writing some new code. 
Simply put, a syntax error is any form of mistake where your code does not follow the [syntax](https://en.wikipedia.org/wiki/Syntax) definitions of the R language.
As a consequence, the code cannot be parsed and executed by the interpreter. 
These error are caught relatively easy, because any syntax error will break the execution of your code.
In addition, when viewing the full Quarto error message (see @nte-rstudio-errors), a syntax error always starts with `Error in parse(text = input): [...]` indicating that _parsing_ of the code failed.

:::{#nte-rstudio-errors .callout-note appearance='simple'}
### Rstudio VS Quarto syntax errors
The syntax errors you see in RStudio will look slightly different from what you see in this book (see @fig-rstudio-error). This has to do with a difference between how RStudio and Quarto (used to create this book) read R code: RStudio reads line by line, Quarto reads an entire chunk. A scenario where this becomes relevant is when you try to render a notebook that contains a syntax error: then you will see the _Quarto_ error, and not the _RStudio_ error.

![Example RStudio error, compare with syntax error example 1 below: the exact error messages are different](rstudio_error.png){#fig-rstudio-error width=60%}

Replicating the Quarto error message in RStudio is possible, but requires some trickery: we force RStudio to read an entire chunk at once.

```{r}
#| eval: FALSE
# Executing this code in RStudio mimics how Quarto parses code and should produce the same error.
code <- "
# Syntax error example 1, missing closing bracket
collection <- c(1,2,3,
"
parse(text = code)
```
:::

```{r}
#| error: TRUE
# Syntax error example 1, missing closing bracket
collection <- c(1,2,3,
```

Note that in R, code is interpreted and potentially executed on a line-by-line basis. 
Executing syntax error example 2 (below) in a clean environment will crash on the second line, but the first line will already have executed when the crash happens.
As a result, in this example the variable `collection` _will_ exist when the crash happens.


```{r}
#| error: TRUE
# Syntax error example 2, the last line is missing the closing bracket
collection <- c(1,2,3)
collection[1
```
An exception to the line-by-line interpretation and execution is how code blocks are treated. Examples 3 and 4 show examples of syntax errors in code blocks: function definitions and loops are first completely interpreted before they are executed.

```{r}
#| error: TRUE
# Syntax error example 3, using incorrect bracket types
my_function <- function(arg){
  return(arg + 1)
]

```

```{r}
#| error: TRUE
# Syntax error example 4, a syntax error in a loop
for (i in 1:3){
  print(i)
  print('There is a syntax error on this line)
}
```
### Runtime errors {#sec-runtime-errors}
A class of errors that will also break execution of your code are the runtime errors. 
Unlike syntax errors, the code can be parsed and interpreted, but there is an underlying problem when trying to execute the code that causes the interpreter to crash.

```{r}
#| error: TRUE
# Runtime error example 1: mismatched types
1 + "2"
```

```{r}
#| error: TRUE
# Runtime error example 2: trailing commas/empty arguments are valid syntax but can break at runtime
collection <- c(1,2,3,)
```

Runtime errors are defined in code and make your code crash when some checks are failing. 
It is relatively straightforward to implement a check and produce an error (see runtime example 3 below).
We will use this more strategically in @sec-testing-and-validating.

```{r}
#| error: TRUE
# Runtime error example 3: implementing a custom check and producing a runtime error
my_function <- function(arg){
  if (arg > 3) {
    stop("arg must be <= 3, got ", arg)
  }
  print(arg)
}
my_function(2)
my_function(4)
```

### Warnings {#sec-warnings}
In some cases, functions that you use might _warn_ you when you are asking for unexpected behavior. One example we previously encountered are the warning messages of functions being overwritten when loading the `tidyverse` package. Warnings do not terminate your code, so just like logical errors you still get results. Unlike logical errors, you get _some_ indication that there might be something not going as intended. Sometimes warnings can be safely ignored, sometimes they mean something is critically wrong.

```{r}
# Warning example 1: converting strings to number raises a warning, but you still get a result
as.numeric(corre_continuous$species[1:5])
```

:::{#exr-warning}
In the example above (when attempting to convert a character to a number), do you think it is appropriate to raise a warning, or would an error be a better choice? 
:::

Implementing warnings in your own functions can be a very powerful tool to keep track of what is happening in your analysis (see example below).
```{r}
# Warning example 2: raising a warning in a function

f <- function(arg){
  if (arg < 3) warning('arg must be >= 3, got ', arg)
  arg ** 2
}

f(2)
```
It is important to realize that warnings, just like runtime errors (@sec-runtime-errors), are implemented in R code by e.g. package maintainers. This means that whether or not a function raises a warning when something unexpected happens always depends on the function, and whether the function author implemented a check or not.

### Logical errors

In sections @sec-runtime-errors and @sec-warnings we have covered what happens when e.g. a package maintainer implements a check in their code. But what about errors in your code that do not cause an error or raise a warning?
These errors are potentially the most dangerous, because they do not show any direct visual clues that something is wrong. Any behavior of your code that does not result in the desired/expected result can be seen as a __logical error__. This can range from explicit mistakes in an implementation (e.g. messing up a mathematical calculation), to not realizing some hidden behavior of a certain function, to simple typo's in variable names. The example at the start of the chapter (@wrn-analysis-gone-wrong) is an example of logical errors: the code ran to completion and produced results, but due to some mistakes in the implementation the results were incorrect.

The crucial problem here is that there is nothing in the code that checks assumptions or e.g. enforces certain properties of datasets/results. In @exr-implement-warnings and @sec-testing-and-validating we explore in more detail how you can implement some of these checks yourself.

:::{#exr-errors}
### Identify the error
For each of the following 10 exercises, identify the type of error, and fix the code so that it works as expected. The aim of this exercise is to develop some intuition, so first try to do this without executing the code!

```{r}
#| eval: FALSE
# Exercise 1: Calculating average leaf Nitrogen content
mean(corre_continuous$leaf_N
     
# Exercise 2: Finding the most common growth form
mean(corre_categorical$growth_form)

# Exercise 3: Is there a relationship between plant family and leaf N content?
cor.test(corre_continuous$family, corre_continuous$leaf_N)

# Excercise 4: What is the average leaf dry mass?
mean(corre_continuous$leaf_dry_mass, na.rm = TRUE)

# Exercise 5: Select all the entries for the grass family
subset(corre_continuous, family = 'Poaceae')

# Exercise 6: Calculate mean leaf are
mean(corre_continuous$SLA)

# Exercise 7: Select the leaf area for the 1,000th plant
corre_continuous$leaf_area[10000]

# Exercise 8: Compute the average seed dry mass for grasses
subset(corre_continuous, family == 'Poaceae')$seed_dry_mass |> mean(na.rm = TRUE)

# Exercise 9: Compute the mean SLA for a specific family
mean_trait_by_family <- function(dataset, trait){
  mean(dataset[[trait]], na.rm = TRUE)
}
mean_trait_by_family(corre_continuous, 'SLA')

# Exercise 10: log-transform seed dry mass
log(corre_continuous$seed_dry_mass)
```
:::

:::{#exr-implement-warnings}
### Upgrading the implementation of the variance calculation
In @exr-variance you implemented the calculation of the variance. Here, you will make that function a bit safer to use. Add the following checks, decide yourself whether it should be a warning or an error:

- The variance of zero or one numbers is undefined
- The variance of a collection of all the same numbers (e.g. `c(4,4,4)`) is zero

Hints: 

- You can implement a check with `if` and `else`:
  ```{r}
  #| eval: FALSE
  if (condition) {
    # Code to run when condition is TRUE
  } else {
    # Code to run when condition is FALSE
  }
  ```
  In this case `condition` must evaluate to `TRUE` or `FALSE`, e.g. `is.numeric(column)` or `length(input) == 3`. You don't always need the `else` part.
- You can throw an error with the `stop()` function.
- You can raise a warning with the `warning()` function.
:::

## Testing and validating {#sec-testing-and-validating}
In @sec-functions we discussed how and when to use functions. It turns out that by following our rule of thumb for when to create a function^[Is there a logical name for a sequence of steps? Then it should be a function. See @tip-function] we create functions that are often straightforward to _test_. In @sec-testing, we discuss what we mean with _testing_ a function, and we highlight two ways of testing your code. In addition, we briefly discuss the difference between testing and validating in @sec-validating.

### Testing code {#sec-testing}
As an example, we revisit @exr-mean-vs-median-all-traits: we want to compute the difference between the mean and the median many times, so it makes sense to implement this in a function.

```{r}
# Example function that we are going to test
mean_median_difference <- function(input){
  # We compute the absolute difference so it doesn\'t matter which number is bigger
  abs(mean(input, na.rm = TRUE) - median(input, na.rm = TRUE))
}

mean_median_difference(grassland_traits_environment$ld)
```

Our example function does not throw errors or raise warnings, and it produces a result, so far everything looks alright! But apart from just reading and checking the code, we don't really _know_ whether our function behaves as intended. This is where it becomes useful to implement some checks of our code, _using code_. In this book we limit ourselves to [unit testing](https://en.wikipedia.org/wiki/Unit_testing): the process of testing small isolated components (i.e. functions). Approaches for testing integrations or entire systems exist as well, but these are outside of the scope of this course.


The key component of testing functions is to identify a set of inputs for which the output is _known_. This often means coming up with small toy examples. In the case of our example function, the computations are relatively straightforward for small amounts of numbers, so we implement a simple testing procedure below:
```{r}
# We will test our function on a small test dataset for which we know the expected outcome of our function
test_input <- c(1, 2, 2, 3, 7)

# The mean of this collection of numbers is 3, and the median is 2, so the difference should be 1
known_output <- 1

# We run out function on the test input to acquire the test output
test_output <- mean_median_difference(test_input)

# We check if the acquired output matches the expected output
if (test_output == known_output) {
  print('Great success!')
} else {
  stop(paste('Expected', known_output, 'but got', test_output))
}
```
:::{#exr-simple-testing}
### Experiment with the simple testing example
Create a few alternative testing datasets with known outputs, and use them with the simple testing code above. Also try out what happens when the known output does not match the test output.
:::

A careful observer might already have identified that our simple testing procedure that we describe above always does the same steps for every test you would design. This makes it a very good candidate for extracting the logic into a function, which is exactly what the [testthat](https://testthat.r-lib.org/index.html) package does! More precisely, the `testthat` package includes a lot of functionality for quickly and reproducibly writing unit tests, with functions that closely resemble the english language. This latter property makes for very readable testing code:

```{r}
# We implement the same test as before, but now with the testthat package, and including more test cases
library(testthat)

test_that("Correct difference between mean and median is calculated", {
  expect_equal(mean_median_difference(c(1, 2, 2, 3, 7)), 1)
  expect_equal(mean_median_difference(c(1, 2, 3)), 0)
})

```

In addition to testing expected outputs, the `testthat` package makes it very easy to test other behavior of functions as well, for example whether it correctly produces an error when incorrect input data is provided:

```{r}
library(testthat)

test_that("Error is thrown on incorrect input", {
  expect_error(mean_median_difference('String'))
})
```
Note that whereas we are incorrectly using the `mean_median_difference()` function and so it raises an error, this is actually part of the test, and so the _test itself_ is successful.

In summary: __unit testing__ describes a set of tests that are automated in code. Tests are designed to throw an error upon unwanted behavior of the code, so if all tests pass this is a good sign of properly working code.

:::{#exr-warning-debugging}
Note that the _error_ test in the example above raises a _warning_? Can you find out where this warning is coming from and what is causing it? Hint: recall @tip-how-to-debug. Why does the test still pass if a warning is raised? What would be the best way to get rid of this warning: adding additional tests, or changing the function?
:::

:::{#exr-implement-tests}
In @exr-implement-warnings you implemented some warnings and errors for your code that calculates the variance. Write tests with the `testthat` package to verify the calculation, errors, and warnings of your function.
:::

### Validating code {#sec-validating}
In the previous section we looked at formally verifying the correctness of a piece of code. In other words, we answered the question "Does the code do what we programmed it to do?". This an important aspect of producing reliable analysis results, but is not the only thing that matters. In addition to formal correctness, you should also always think about whether it _makes sense_ what your code is doing. In this course, we call this __validating__: "Is the code solving the right problem?".

For validation, we typically do not use automated procedures. Rather, we rely on external comparisons: this can be checking plausability of results with literature, but also submitting your code to expert- or peer review (This is in part why we do weekly code review sessions!). 

Ultimately, you want your code to be both tested and validated (see @tbl-testing-validating), but it takes some time and effort before you get there! 

| |Tested |Untested |
|-|-------|---------|
|__Validated__| Provably correct, relevant results | Correct idea, buggy execution|
|__Unvalidated__| Bug-free implementation of a flawed idea | Don't go here ðŸ’€|
: Different combinations of testing and validating your approach lead to different guarantees on your results {#tbl-testing-validating .bordered .responsive-sm .hover}

:::{#exr-was-it-tested}
Revisit @wrn-analysis-gone-wrong and decide whether testing, validating, or both went wrong. How should plant scientists make sure they do better than this example?
:::
