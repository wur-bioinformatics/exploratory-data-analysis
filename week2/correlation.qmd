# Correlation {#sec-correlation}

```{r setup}
#| echo: FALSE
# this code is only for getting the data into the notebook during publishing
data(corre_continuous, package = "planteda")
data(grassland_traits_environment, package = "planteda")
```

In this chapter we cover a fundamental approach for comparing two variables: correlation. Calculating correlation is often an important part in the data analysis pipeline (@tip-data-science-cycle). Just like summary statistics, correlation can be relevant in both the inspecting and analyzing parts of the pipeline. Since correlation is about _comparing_ things, it is critical for the interpretation of a correlation analysis that it is clear what is being compared. 

We first introduce the concept of _co_-variance, from that we derive linear and non-linear correlation measures. We then briefly discuss statistical testing of correlation measures, and we finish with testing and interpreting a correlation analysis (and how to use correlation in building an argument).

## Covariance {#sec-covariance}
In @sec-variance we introduced the concept of _variance_. By calculating the variance you can answer the following question: how much do individual observations differ from the mean? In this chapter we are interested in comparing two variables, and so we generalize the notion of variance to _co_-variance. By calculating the covariance you can answer the following question: if one variable variable varies by some amount, how much does my second variable _co_-vary? In other words: if one variable is high, is my other variable high as well? In a concrete example: do plants that have a large specific leaf area also have a high leaf nitrogen content?

The mathematical derivation for the covariance is relatively straightforward and directly builds on the definition of variance (@eq-variance2). 

$$
Variance(X) = \frac{1}{N}\sum_{i=1}^{n}{(\mu-x_i)^2}
$$ {#eq-variance2}

We start by writing the squared difference from the mean as explicit multiplication (@eq-variance-expanded).

$$
Variance(X) = \frac{1}{N}\sum_{i=1}^{n}{(\mu-x_i)(\mu-x_i)} 
$$ {#eq-variance-expanded}

Next we swap out one of the difference terms with the difference from the mean of another variable. Note that we use the respective means for the two variables: $\mu_x$ and $\mu_y$ (@eq-covariance)! 

$$
Covariance(X, Y) = \frac{1}{N}\sum_{i=1}^{n}{(\mu_x-x_i)(\mu_y-y_i)}
$$ {#eq-covariance}

The R language has a built-in function for computing the covariance:
```{r}
# Do plant species with a high specific leaf area have a high leaf nitrogen content?
cov(corre_continuous$SLA, corre_continuous$leaf_N, use='complete.obs')
```

:::{#exr-compare-variance-covariance}
### Comparing variance and covariance
The formulas in @sec-covariance derive how covariance is a generalization of variance. In this assignment you will verify this empirically using R's built-in `var` and `cov` functions.
For a variable of your choice, compute the it's variance, and the covariance of that variable _with itself_. Compare the outputs. How do you explain this? Hint: compare @eq-covariance with @eq-variance-expanded.
:::

:::{#exr-covariance-units}
### Interpreting the units of covariance
As discussed in @sec-stddev, the units of variance are the square of the units of measurement. Verify how you can derive this from the mathematical formulation of the variance (@eq-variance2). Next, describe what the units of covariance are. How interpretable do you think the units of covariance are? Hint: identify what the units of the two separate elements of the product term inside the sum are, and compare those to the units of measurement.
:::

## Linear Pearson correlation
In @sec-stddev we discussed how the standard deviation solves a problem of the variance: it expresses the units in which we express variation in the same units as the mean and the original observations. _Correlation_ solves a similar problem that occurs with the covariance, but in a slightly different way. In @exr-covariance-units you worked on the units of covariance, and from that is should become obvious that it is not straightforward to transform the units of covariance into the original units of measurement. In addition, the covariance can take on values in the range `-inf` to `+inf`. The __pearson correlation coefficient__ solves some of these issues: it _normalizes_ the covariance to be restricted to the range `-1` to `+1`, and as a side-effect is a _dimensionless_ quantity.

To normalize the covariance, the pearson correlation coefficient divides the covariance by the product of the squared variances (@eq-pearson-full).

$$
PearsonCorrelationCoefficient(X,Y) = r{X,Y} = \frac{Covariance(X,Y)}{\sqrt{Variance(X)}\sqrt{Variance(Y)}}
$$ {#eq-pearson-full}

This is also often expressed in terms of standard deviations $\sigma$:
$$
r_{X,Y} = \frac{Covariance(X,Y)}{\sigma_X \sigma_Y}
$$ {#eq-pearson-short}

:::{#nte-pearson-properties .callout-note appearance='simple'}
### Useful properties of the pearson correlation coefficient
The pearson correlation coefficient $r$ has a few useful properties that help interpretation.

- __Range__ `-1` to `+1`: correlation coefficients of 1 (either positive or negative) indicate a _perfect_ relationship (in other words, all values are exactly equal). A correlation coefficient of 0 indicates _no_ relationship.
- The __sign__ indicates the direction of the relationship: positive coefficients indicate a high value in one variable corresponds to a high value in the other variable, negative coefficients indicate a high value in one variable corresponds to a low value in the other variable.
- It measures __linear__ relationships. In other words: it draws a _straight line_ through the datapoints. There is a strong connection with Ordinary Least Squares regression, in this course we do not go into detail about this relationship.
- It is __dimensionless__: since it has no units, and the range is always the same, the interpretation of the correlation coefficient is always the same. In other words: the interpretation of the correlation coefficient doe s not depend on the units of the measured variable
:::

:::{#exr-verify-pearson}
### Verify the calculation of the pearson correlation coefficient.
@eq-pearson-short describes the pearson correlation coefficient in terms of covariance and standard deviations. Pick two variables of your choice in the `corre_continuous` dataset and verify the output of `cor()` with calculating the coefficient yourself using `cov()` and `sd()`.
:::

:::{#exr-plotting-pearson}
Since the pearson correlation coefficient describes a linear relationship, it is relatively straightforward to draw this relationship as a straight line in a scatterplot. Using `ggplot`, create a scatterplot of your two variables of interest with `geom_point` and add the correlation line using `geom_smooth`. Make sure to use `method = 'lm'` for `geom_smooth`. What is the relationship between the pearson correlation coefficient and the line you've drawn?
:::

## Non-linear Spearman correlation {#sec-spearman}
Not all real data is best described by a straight line (@fig-correlation)! We will not go into too much detail on how to express non-linear relationships here (there are many options, most are outside the scope of this course). However, a relatively straightforward modification of the spearman correlation coefficient makes it somewhat suitable to express non-linear relationships. The _spearman correlation coefficient_ is a non-linear correlation approach that simply calculates the pearson correlation coefficient on the _ranks_^[i.e. what is the first biggest number, and the second biggest, etc.] of the variables. In doing so, spearman correlation effectively ignores the magnitude of the differences in a variable: big jumps are just as important as small jumps. Spearman correlation keeps most of the properties of pearson correlation (@nte-pearson-properties ) except one: the relationship is no longer linear, but instead _monotonic_: the variables consistently increase or decrease together, without requiring a straight-line relationship.

```{r}
#| echo: FALSE
#| warning: FALSE
#| error: FALSE
#| label: fig-correlation
#| fig-cap: "The relationship between Specific Leaf Area and Leaf Dry Matter Content in the CoRRE dataset. Green line represents a linear model fit and is directly related to the pearson correlation between these two variables. Blue line represents a non-linear model fit and is probably a better fit for this relationship. Note: the blue line is _not_ spearman correlation, but based on a different non-linear model fit that is outside the scope of this book."
library(ggplot2)

ggplot(na.omit(corre_continuous), aes(x=SLA, y=LDMC)) + 
  geom_point(alpha=.5) + 
  geom_smooth(method='gam', se=FALSE, color = "#0A89FF") + 
  geom_smooth(method='lm', se=FALSE, color = "#07B07B") + 
  guides() +
  theme_bw() +
  labs(
    title = "Relationship between Specific Leaf Area and Leaf Dry Matter Content",
    x = "Specific Leaf Area [mm2/mg]",
    y = "Leaf Dry Matter Content [g/g]" 
  )
```
:::{#exr-spearman}
### Verifying the spearman correlation coefficient
@sec-spearman describes how non-linear spearman correlation is a simple modificiation of pearson correlation. Verify this is true: you can get the ranks of a variable by using the `index.return = TRUE` option of the `sort()` function. Pick two variables of your choice, and compare spearman correlation (`cor()` function with `method = spearman`) to pearson correlation on the ranks.
:::

:::{#exr-spearman-vs-pearson}
### Interpreting the difference between Spearman and Pearson correlation
For two variables of your choice, compute both the spearman and pearson correlation coefficient. Are they different? How different are they? What do you think this means?
:::

:::{#exr-multiple-correlations}
### Computing many correlations on the same dataset
For datasets with multiple quantitative variables, a straightforward question is to ask 'which variables are correlated with each other?'. The built-in `cor()` function makes use of vectorization (See @sec-vectorization) to make this a straightforward question to ask.

- Select all numeric columns of the `grassland_traits_environment` dataset:
  ```{r}
  #| eval: FALSE
  numeric_columns <- sapply(grassland_traits_environment, is.numeric)
  grassland_numeric_df <- grassland_traits_environment[numeric_columns]
  ```
- Compute pairwise correlations of all variable pairs:
  ```{r}
  #| eval: FALSE
  cor(grassland_numeric_df, use = 'complete.obs')
  ```

What does the output look like? How do you interpret all these numbers? What does `use = "complete.obs"` do (Hint: check the documentation with `?cor`)?
:::

## Testing and interpretation (and how to build an argument)
From @nte-pearson-properties it has become clear that a correlation coefficient of zero indicates _no relationship_. But when do we call a non-zero correlation coefficient sufficiently large to indicate a _significant_ relationship? This is a question that typically lends itself for a statistical approach! In @sec-correlation-testing we will not go into too much detail, but a brief description of how significance testing for correlation coefficients work is necessary to interpret results. Once we have established how to determine which correlation coefficients can be seen as _statistically significant_, we ask the question "how to interpret a significant correlation?" in @sec-correlation-interpretation. 

### Testing {#sec-correlation-testing}
The main points for statistical testing of correlation coefficients are bundled in @nte-correlation-testing. 

:::{#nte-correlation-testing .callout-note appearance='simple'}
### Correlation coefficient significance testing
To test whether a correlation coefficient is (statistically) significantly different from zero, we take into account three things:

- __Sample size__ (n): with larger samples we are more confident our observation is 'real'
- __Magnitude__ of the correlation coefficient: a larger coefficient is more likely to be significant
- Chosen __significance level__ ($\alpha$): the typical statistical approach, we test a null hypothesis $H_0: r = 0$, if the p-value is smaller than $\alpha$ we conclude significance.

For __Pearson correlation__ the test looks as follows: we compute a t-statistic based on sample size $n$ and correlation coefficient $r$, and we compare the t-statistic to a t-distribution to get a p-value. Approaches for other correlation coefficients are similar in nature, but differ in details.
$$
t = r\sqrt{\frac{n-2}{1-r^2}}
$$ {#eq-pearson-test}

From @eq-pearson-test it can be observed that a large sample size $n$ will _always_ lead to a large test statistic $t$, which in turn will _always_ lead to a low p-value. @tbl-pearson-significance provides a few examples of which correlation coefficient will be significant at various sample sizes.

| Sample size | Smallest \|r\| that is significant |
|------------:|---------------------------------:|
| n = 10 | ≈ 0.63 |
| n = 30 | ≈ 0.36 |
| n = 100 | ≈ 0.20 |
| n = 1000 | ≈ 0.06 |
: Pearson $r$ significance levels at different sample sizes using $\alpha=0.05$ {#tbl-pearson-significance .hover .responsive-sm}

The __take home message__ here: A correlation is significant when the data provide enough evidence that the true association is not zero. This depends more on sample size than on the strength of the relationship!
:::

:::{#exr-exploring-anscombe}
### Exploring Anscombe's dataset
In this assignment you'll explore the limitations of summary statistics and correlation. Understanding these limitations is very important when analysing real datasets. To make the limitations very explicit, you will use a artificial dataset designed in 1973 by statistician Francis Anscombe. There are four pairs of observations in this dataset, so it's often referred to as 'Anscombe's quartet'. You can load the dataset into your R session with `data(anscombe)`, the corresponding dataframe will be called `anscombe`.

- Calculate the main summary statistics discussed in @sec-summary-statistics using the `summary()` function. What do these look like?
- Calculate correlation coefficients for all pairs of `X` and `Y`, check both Pearson and Spearman. What do these look like? 

After calculating the summary statistics, let's take a look at a plot of these four pairs of observations. Copy-paste the code below^[We will cover 'long' and 'wide' dataframes and how to convert between these in the next chapter] and run it in your R session. What do you observe? How does this affect the interpretation of the summary statistics and correlation coefficients?

```{r}
#| eval: FALSE
# Plotting anscombes quartet
library(ggplot2)

# Load the built-in dataset
data(anscombe)

# Reshape to long format
anscombe_long <- data.frame(
  x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4),
  y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4),
  set = factor(rep(1:4, each = nrow(anscombe)))
)

# Plot
ggplot(anscombe_long, aes(x, y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(~ set, ncol = 2) +
  labs(
    title = "Anscombe's Quartet",
    x = "x",
    y = "y"
  ) +
  theme_minimal()
```
:::

### Interpreting correlations (and building strong arguments) {#sec-correlation-interpretation}
The main interpretation of finding a (significant) correlation between two variables, is that these two variables are _somehow_ related. This might seem obvious at first, but there are a few subtle nuances that are important to realize that might lead to _over interpretation_ when ignored. The most important nuance of correlation, is that it is not causation (See @wrn-causal-correlation).

:::{#wrn-causal-correlation .callout-warning appearance='simple'}
### Correlation is not causation
Correlation describes __that__ two variables are related, it does _not_ tell you __why__ the variables are related. In many cases you will be interested in asking the 'why' question, in which case correlation might give you a hint. However, a correlation coefficient alone is never enough to make a causal statement.

A good visual aid in remembering this, is that correlation is an _undirected_ relationship, whereas causation describes a _directed_ relationship: note the arrow vs straight line in @fig-correlation-vs-causation below.

![Correlation is undirected, causation implies a direction.](correlation_vs_causation.png){#fig-correlation-vs-causation width=.5}

:::

There are many examples of correlations that are not causal (also called _spurious_ correlations), for example those collected by [Tyler Vigen](https://www.tylervigen.com/spurious-correlations). Perhaps counter intuitively, there are also clear cases where a causal relationship does _not_ lead to a correlation (See @fig-photosynthesis for an example in the plant sciences).

![Light intensity and photosynthesis efficiency are clearly causally related, but show no inherent correlation when measured: the relationship is neither linear nor monotonic. Figure from [-@Benedetti2018]](photosynthesis.png){#fig-photosynthesis width=60%}

:::{#tip-building-correlation-arguments .callout-tip appearance="simple"}
### Building a scientific argument based on correlation
A strong scientific argument based on correlation builds on a few principles, which are outlined here. Building scientific arguments _in general_ is a fundamental aspect of good science, but outside of the scope of this course.

When describing correlations, consider the following:

- A high correlation supports an argument, but never proves a causal mechanism (See @wrn-causal-correlation).
- Inspect the scatterplot: @exr-exploring-anscombe provides clear examples of what can go wrong.
- Check you assumptions: linear versus non-linear? What do you know about the data that would suggest either of these to be a better fit?
- Effect size is more important than significance: with enough observations, _every_ correlation is significant.
- Robustness: what happens if you remove obvious outliers? 
- Provide additional evidence: are there existing theories or mechanisms known?

__Take home message__: A correlation coefficient is evidence, not a verdict. Strong scientific arguments combine visual inspection, use appropriate methods, consider effect size, and include (biological) reasoning about the context.
:::

:::{#exr-interpret-many-correlations}
Revisit @exr-multiple-correlations: Are there any groups of variables that are all correlated to eachother? How would you interpret such a group of variables? Which of the steps of @tip-building-correlation-arguments can you apply, and which are not feasible? How does this affect your conclusions?
:::
