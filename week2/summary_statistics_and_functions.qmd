---
title: "Summary statistics and functions"
format: html
---

## Introduction

A simple but effective way of starting to explore a dataset is to compute a few summary statistics.
In the previous week we already encountered a few summary statistics: we looked at e.g. the numbers of rows and columns of a table.
In this section we will explore a few numerical summary techniques, such as computing the mean and standard deviation of a numeric variable, or counts/frequencies of a categorical variable.
At the same time, we will look into one of the more powerful features of using a programming language: the possibility to create functions.

```{r}
remotes::install_github('wur-bioinformatics/planteda')
library(planteda)
data('corre_continuous')
```

## Summary statistics

Perhaps one of the simplest and most intuitive summary statistics is the mean (typically indicated with the greek letter $\mu$).
The mean can be calculated as the sum of all values in a variable divided by the number of values in that variable (@eq-mean).

$$
Mean(X) = \mu = \frac{1}{N}\sum_{i=1}^{n}{x_i}
$$ {#eq-mean}

The simplest and most direct interpretation of the mean is that it represents the average value of a variable.
A slightly more statistically oriented interpretation is that the mean represents the expected value.
In other words: if you have no other information, the mean would be your best guess^["Best guess" is kept informal here. A full probabilistic treatment exists, but is beyond the scope of this book] on average.
In the example below this would mean that your best guess of the leaf nitrogen content of a plant, without knowing anything else about that plant, would be 22.9.

Being a statistical programming language, R comes with a default function to calculate the mean:

```{r}
# Calculate the average leaf nitrogen content
mean(corre_continuous[['leaf_N']])
```

::: {.callout-note appearance="simple"}
### Implementing the mean calculation in a custom function

Since R already comes with a default function to calculate the mean this example might seem redundant. At the same time, this gives us the opportunity to check whether our custom implementation returns the correct result.

```{r}
# Here we define a function to perform the calculation of the mean, 
# closely following the mathematical description
custom_mean <- function(input){
  total = sum(input)
  number = length(input)
  return(total / number)
}

# This should give the same result as using the default `mean` from R
custom_mean(corre_continuous[['leaf_N']])
```
:::

Where the mean describes the average value of a variable, the variance describes how much individual data points differ from the mean.
More precisely, the variance is defined as the average of the squared differences between each observation and the mean (@eq-variance).
In other words: for each value we compute how far it is from the mean, square this difference, and then average these squared deviations.
In our example on leaf nitrogen content, the variance quantifies how much individual plants differ from the average leaf nitrogen content.
Note that squaring the difference has two important consequences: positive and negative differences contribute equally, and large differences contribute relatively more than small differences.
A consequence of this last property is that the variance is relatively sensitive to outliers in the data.
$$
Variance(X) = \sigma^2 = \frac{1}{N}\sum_{i=1}^{n}{(\mu-x_i)^2}
$$ {#eq-variance}

One slight downside in interpreting the variance is that the _units_ are squared compared to the data points.
In our example, leaf nitrogen content is measured in _g/g_, and as a consequence the means is also in _g/g_.
However, the variance is in _(g/g)^2_, which makes interpretation less straightforward.
A common and useful solution is to take the square root of the variance, which results in the standard deviation(@eq-standard-deviation).

$$
StandardDeviation(X) = \sigma = \sqrt{\frac{1}{N}\sum_{i=1}^{n}{(\mu-x_i)^2}}
$$ {#eq-standard-deviation}

::: {#exr-variance}
### Implement the variance calculation in a custom function
Just like in the previous example on the mean, R has a built-in function to calculate the variance (`var`).
To practice implementing custom functions, in this exercise you will implement the calculation of the variance yourself.
Compare the output of your custom function to the output of the built-in `var` function to check your work.

```{r}
#| eval = FALSE
# Implement the calculation of the variance in a custom function
custom_variance <- function(input){
  # ... some initial calculation goes here
  variance <- # ... final calculation goes here
  return(variance)
}
```
:::

The mean and variance summarize the data by combining all values using arithmetic operations.
In contrast, some summary statistics depend only on the _relative order_ of the values, not on how large or small individual observations are.
These order-based statistics often behave more robustly when extreme values are present.
In the following section we discuss the median, its generalization into quantiles, and the accompanying interquartile range.

The simplest and most direct interpretation of the median is that it represents the middle value of a variable.
More precisely, the median is the value that splits the data into two equally sized parts when the observations are sorted.
In other words: half of the observations are smaller than the median, and half are larger.
In the example below this would mean that half of the plants have a leaf nitrogen content below the median value of 21.7, and half above.

```{r}
# Calculate the median leaf nitrogen content
median(corre_continuous[['leaf_N']])
```
Quantiles generalize the idea of the median by describing values at fixed positions in the sorted data.
A q-quantile is the value below which a fraction $q$ of the observations fall.
In other words: a q-quantile splits the data into a lower part containing a fraction $q$ of the observations and an upper part containing the remaining fraction.
For example, the 0.25-quantile (also called the first _quartile_) is the value below which 25% of the data lie.

A commonly used measure of spread in the data based on quantiles is the interquartile range (IQR).
The IQR is defined as the difference between the third quartile and the first quartile.
In other words: it measures how much the middle 50% of the data vary.
In the example below this would quantify the spread of leaf nitrogen values for the central half of the plants.

```{r}
x <- corre_continuous[['leaf_N']]
percentile <- 0.5
percentile_index <- length(x) %/% (1 / percentile)
sort(x)[percentile_index]
```
```{r}
library(ggplot2)
library(grid)

specific_leaf_area <- na.omit(corre_continuous$SLA)

m <- mean(specific_leaf_area)
s <- sd(specific_leaf_area)
qs <- quantile(specific_leaf_area, c(0.25, 0.75))

stats <- c(
  mean = m,
  mean_plus_sd = m + s,
  mean_min_sd  = m - s,
  median = median(leaf_dm),
  q1 = qs[[1]],
  q3 = qs[[2]]
)

line_data <- data.frame(
  stat = names(stats),
  value = unname(stats),
  legend = c("Mean", "SD", "SD", "Median", "IQR", "IQR"),
  row.names = NULL
)

line_data$legend <- factor(
  line_data$legend,
  levels = c("Mean", "SD", "Median", "IQR")
)

ggplot(data.frame(leaf_dm), aes(x = leaf_dm)) +
  geom_density(fill = "lightgrey", alpha = 0.3, adjust = 5) +
  geom_rug(alpha = 0.1) +
  
  geom_vline(
    data = line_data,
    aes(
      xintercept = value, 
      linetype = legend, 
      colour = legend),
    linewidth = 1
  ) +
  
  scale_linetype_manual(
    values = c(
      Mean    = "solid",
      SD      = "3113",
      Median  = "42",
      IQR     = "11"
    )
  ) +
  
  scale_colour_manual(
    values = c(
      Mean    = "purple",
      SD      = "purple",
      Median  = "darkgreen",
      IQR     = "darkgreen"
    )
  ) +
  
  guides(
    colour = guide_legend(
      override.aes = list(linetype = c("solid","3113","42","11"))),
    linetype = "none"
  ) +
  
  labs(
    title = "Distribution of Specific Leaf Area",
    x = "Specific Leaf Area",
    y = "Density",
    colour = "Statistic"
  ) +
  theme_minimal()

```
