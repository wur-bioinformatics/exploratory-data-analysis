---
title: "From messy to clean"
author: "Mark Sterken"
date: "2025-12-24"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
editor: visual
execute:
  warning: false
  message: false
---

## From messy to clean

### Introduction to day 2

Today we will continue with goal 1 (Built basic knowledge on using R and R studio) and start with goal 2 (Be able to recognize and load common data formats) and goal 3 (Apply common statistical tools for inspecting and analyzing data)

We will make use of a fragmented simulated dataset. You need to puzzle it together using a couple of functions. Therafter you can analyze it further.

::: {.callout-tip appearance="simple"}
#### Potato cyst nematodes, virulent *Globodera pallida* within North-West Europe

One of the current projects that is running at [the Laboratory of Nematology](https://www.wur.nl/en/chair-groups/laboratory-of-nematology), is an investigation into the virulence of the potato cyst nematode *Globodera pallida*. Although plant susceptibility is a complex traits, there are 'Mendelian nuggets' to be found that control these parasites. These 'Mendelian nuggets' are loci (e.g. one or a couple of genes) that have a strong effect on a trait. In this case, a particular resistance gene that was bred into potato and which strongly controls the reproduction of *G. pallida*.

A PhD in our group, Arno Schaveling (who studied Plant Science at WUR) found out that the Resistance gene GpaV~vrn~ was broadly introduced into potato. This led to massive, parallel selection for virulence of *G. pallida* in the field[@Schaveling2026]. Currently resulting in a virulence outbreak in North-West Europe[@Schaveling2025].

In particular, you can select for virulence in *G. pallida* over subsequent generations in one potato cultivar. Thereafter the ability of the resulting populations to reproduce in many potato cultivars is enhanced. This result meant that the resistance base in potato was relatively narrow.

![Selection on the GpaV~vrn~-containing potato variety Seresta also increases virulence on other commercial potato varieties. (a) The selection experiment that was carried out with four generations of selection on Seresta. The colours indicate the potato variety used (Red Desiree, Orange Seresta), and the circles with the numbers indicate which propagation was used in the standard potato cyst nematode (PCN) resistance test. The numbers indicate the generation of selection. Due to the amount of material required for the standard PCN resistance test, Generation 0 and Generation 1 could not be derived from the selection line Generations 2--4 were on. (b) The outcome of the standard PCN resistance test for selected Generations 1--4. Each dot represents the reproduction (P~f~/P~i~) measured in one 2-l pot inoculated with 10000 larvae. The lines are linear regressions added as a visual aid. Colours indicate varieties, where Seresta and Innovator belong to Cl~SER~ and Libero, Avarna and Festien belong to Cl~FES~. The R~2~ is the amount of variance explained by generation as derived from a PERMANOVA.](Week1_day2_Schaveling1.jpeg)

Next, we were able to identify a region on the *G. pallida* genome that was selected, ultimately identifying the gene *Gp-pat-1*. This gene is currently used as the target for a molecular assay to detect virulence in agricultural fields.

![Single genetic locus is selected after repeated selection on Seresta. (a) Overview of the AMPOP02 (dots) and AMPOP10 (triangles) samples generated in the selection experiment and how many were successfully sequenced. As the larvae from which we isolated DNA underwent either a selection on Desiree (red) or Seresta (orange), we took the final number of generations selected on Seresta along in the analysis. (b) The number of variants that significantly correlate with virulence per scaffold plotted against the scaffold size in million bases (Mb). Dots indicate the variants found in AMPOP02, and triangles indicate variants found in AMPOP10. Text indicates which scaffold of the Rookmaker genome the datapoint belongs to. (c) The significance of the variants on scaffold 28 associated with generation in AMPOP10. The variants are plotted on their location on scaffold 28 (in Mb) vs the significance in −log~10~(P). The variants where the frequency of the alternative allele is decreasing over generations are coloured red, while those where the alternative allele is increasing are coloured blue. Grey variants are not significant. The dashed horizontal line indicates the Bonferroni-corrected threshold. The dashed vertical lines indicate the candidate locus. Note that the y-axis has been cut off at −log~10~(P) \< 2.](Week1_day2_Schaveling2.jpeg)
:::

#### Set up your R environment

For the basics of setting up your environment, you can check Chapter 1. The Markdown file to use for the exercises and documentation of your code is here:

```{r w1d2_script, eval=FALSE, class.source = 'fold-show'}

# BIF20806 - Day 2 ---------------------------------



# set a work directory ----------------------------------

    ###this function you can use to automatically set a location
    ###see tutorial 1, day 1 for more instructions
    #setwd("D:/")
    


# Install packages ----------------------------------------------------

    ###see tutorial 1, day 1 for more explanation

# Load data -----------------------------------------------------------


# Clean data ----------------------------------------------------------
   
    ### Inspect the data ##############################################


    ### Repair errors and remove outliers #############################

# Analyzing -----------------------------------------------------------
        
    ### Normal distribution ###########################################


    ### ANOVA #########################################################


    ### Post-hoc test #################################################


    ### Simple linear model ###########################################


# Presenting ----------------------------------------------------------

    ### Simple box-plot ###############################################


    ### A bit more fancy box-plot #####################################


# The Van Sluijs dataset ----------------------------------------------


```

#### Loading data into R

In this tutorial, we will start with loading different types of data files into R. This is an essential skill for working with real-world data. We'll cover a couple of file-types that are common. *Each of you has their data in these formats*, you can find these via the fileshare on [Brightspace](https://brightspace.wur.nl/). If you are familiar with R, this part might be easy.

-   Excel files (`.xlsx`)
-   Comma-separated values (`.csv`)
-   Semicolon-separated CSV files (`.csv`)
-   Tab-delimited text files (`.txt` or `.tsv`) (you encountered this filetype yesterday)

To make this part of the course realistic, *we have provided you with data in these four formats*. The reason being that this is also how it works in practice. Finding out the format of the data and loading it into memory is a crucial step in using any data-analysis pipeline.

You now need to load in data from all four data formats.

##### Loading data from an excel file (.xlsx)

To read Excel files, we use the `read_excel()` function from the readxl package. Excel files are very common but also very tricky. Namely, these files are mostly made by persons and people *do not automatically follow conventions* that are clear for a programming language. Therefore always check the excel file before loading it in.

```{r w1d2_load_xlsx, include = TRUE,eval=FALSE, class.source = 'fold-show'}
#| code-fold: false

    ### Package
    install.packages("readxl")    # For reading Excel files
    library(readxl) # first activate the package

    # Replace with the path to your Excel file
    data_excel <- readxl::read_excel("data/YOURFILENAME.xlsx")

```

##### Loading data from a CSV File (Comma-separated)

CSV files are very common as format for data. Here each column is separated by a comma, rows are typically observations. We use the `read.csv()` function to load the data.

```{r w1d2_load_csv, include = TRUE,eval=FALSE, class.source = 'fold-show'}
#| code-fold: false

    # Replace with the path to your csv file
    data_csv <- read.csv("data/YOURFILENAME.csv")

```

##### Loading data from a CSV file (semicolon-separated)

Some CSV files (especially from European systems) use a semicolon (;) instead of a comma. Use `read.csv2()` for these files

```{r w1d2_load_csv2, include = TRUE,eval=FALSE, class.source = 'fold-show'}
#| code-fold: false

    # Replace with the path to your csv file
    data_csv2 <- read.csv2("data/YOURFILENAME.csv")

```

##### Loading a Tab-delimited File

These files use tabs to separate values. They often have a .txt or .tsv extension.

```{r w1d2_load_tabdelim, include = TRUE,eval=FALSE, class.source = 'fold-show'}
#| code-fold: false

    # Replace with the path to your csv file
    data_tsv <- read.delim("data/YOURFILENAME.txt")

```

### Analysis of (a)virulence of *Globodera pallida*

#### Setup R

Before we start, make sure you have the following packages ready: {tidyverse}, and {ggpubr}. Of course you also need to activate them. If you're not sure how, check [Chapter 1](/week1/day1.qmd).

```{r w1d2_packages_readxl}
    ### type the code to install the packages

    ### type the code to activate the packages
```

#### Inspecting data part 1

##### Inspecting data using functions

Last time we used `head()`, `tail()`, and `summarise()`. Again apply these functions. But now we also want to check which piece is which. For that the functions `length`, `ncol()`, `nrow()`, and `dim()` are handy.

```{r w1d2_check_the_data_ASSIGNMENT, include = TRUE,eval=FALSE}

    # Check the first 6 rows
    head(data_csv)

    # Check the last 6 rows
    tail(data_csv)
    
    # get a summary of the data
    summary(data_csv)
 
    # get the length of a vector
    length(data_csv)
    
    # get the number of columns
    ncol(data_csv)
    
    # get the number of rows
    nrow(data_csv)
    
    # get the dimensions of an object
    dim(data_csv)
    
```

##### Inspecting data using histograms

Like last time, plot a histogram using [ggplot2](https://ggplot2.tidyverse.org/).

```{r w1d2_make_a_ggplot2_histogram_ASSIGNMENT, include = TRUE,eval=FALSE}

        ggplot2::ggplot(YOUROBJECT,aes(x=Gpal_vir)) +
        geom_histogram()

        ggplot2::ggplot(YOUROBJECT,aes(x=Gpal_avir)) +
        geom_histogram()
    
```

#### Cleaning and preparing the data

Now you have the dimensions of the puzzle pieces. You can combine them in only one way. First you need to join the two pieces (1 and 2) with the equal number of rows using `cbind()`. Second, you need to add the piece (3) that now has the same number of columns using `rbind()`. Finally, you need to add the last piece (4) of data using `merge()`.

```{r w1d2_combine_pieces_ASSIGNMENT, include = TRUE,eval=FALSE}

        ### bind objects together over columns
        pheno_data <- cbind(YOURPIECE1,YOURPIECE2)
        
        ### binds objects together over rows
        pheno_data <- rbind(pheno_data,YOURPIECE3)
        
        ### merges objects, seeks for the same identitier, or you can provide it using by.x and by.y
        pheno_data <- merge(pheno_data,YOURPIECE4)
    
```

After combining, you can inspect the data again, for instance using `geom_histogram()`.

#### Analyzing

##### Normal distribution

Same as before, let's figure out whether the data is normally distributed. For this we can use the functions `stat_qq()` and `stat_qq_line()`.

```{r w1d2_qqplot_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(pheno_data,aes(sample=Gpal_vir)) + 
    stat_qq() + stat_qq_line()

    ggplot(pheno_data,aes(sample=Gpal_avir)) + 
    stat_qq() + stat_qq_line()

```

Of course, you can also use a Shapiro-Wilk test (`shapiro.test`), this calculates the likelyhood whether the data follows a normal distribution.

```{r w1d2_shapiro_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    shapiro.test(pheno_data$eggmass)

```

##### Wilcoxon test

As the data is not exactly normally distributed, let's deal with that in the proper way. Now we can use a Wilcoxon Rank Sum test to compare between two samples. We first need to transform the dataset, as we do not have a single observation per row currently. For this you can use the `gather()` function from the {tidyr} package. Use functions like `head()` and `tail()` to check what happened.

```{r w1d2_gather_pallida, include = TRUE,eval=FALSE, class.source = 'fold-show'}

    data.test <- tidyr::gather(pheno_data,key="pallida",value="juveniles",-ID,-Experiment)

```

Now we can test how the potato plants performed against virulent versus avirulent *G. pallida*. Using `wilcox.test()`

```{r w1d2_Wilcox_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    wilcox.test(juveniles~pallida,data=data.test)
    ###you should have a very low p-value

```

#### Presenting

Can you now also make a plot of what you just tested? Use `geom_boxplot()` to visualize the distribution and `geom_jitter()` to show the individual measurements.

```{r w1d2_wilcoxplot_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(data.test,aes(x=pallida,y=juveniles)) +
    geom_boxplot() + geom_jitter(height=0,width=0.25)


```

There is another variable in the data as well 'Experiment'. Use `facet_grid()` to split this out. Figure out how using the [ggplot2 website](https://ggplot2.tidyverse.org/).

```{r w1d2_wilcoxplot_facet_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(data.test,aes(x=pallida,y=juveniles)) +
    geom_boxplot() + geom_jitter(height=0,width=0.25) +
    facet_grid(~Experiment)

```

Now we can add p-value as well using `annotate()`.

```{r w1d2_wilcoxplot_pval_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ###I'm getting the statistics here
    tmp <- tapply(data.test[,c("pallida","juveniles")],data.test$Experiment,function(x){
                                broom::tidy(wilcox.test(x$juveniles~x$pallida))})
   
    statplot <- do.call(rbind,tmp) %>%
                mutate(Experiment=1:3,pallida=1.5,juveniles=max(data.test$juveniles),
                       label=paste("p =",signif(p.value,2)))

    p1 <- ggplot(data.test,aes(x=pallida,y=juveniles)) +
          geom_boxplot() + geom_jitter(height=0,width=0.25) +
          facet_grid(~Experiment) + geom_text(aes(label=label),data=statplot)

```

##### Saving plots

Now save the plot, check back at Tutorial 1

```{r w1d2_pdf_of_pallida_plot, include = TRUE, eval=FALSE}

    pdf(file="Figure_pallida.pdf",width=3,height=4)
        print(p1)
    dev.off()   
        
```

#### Inspecting data part 2

##### Correlation analysis

As the data is not exactly normally distributed, we also have to deal with that when calculating correlation. For normal (distribution) correlation, you should use the Pearson method. This is the standard setting for `cor()`. We first need to transform the dataset, as we do not have a single observation per row currently. For this you can use the `gather()` function from the {tidyr} package. Use functions like `head()` and `tail()` to check what happened.

```{r w1d2 correlation analysis ASSIGNMENT, include = TRUE,eval=FALSE}
    
    data.test <- pheno_data

    ### Pearson correlation
    cor(data.test$Gpal_vir,data.test$Gpal_avir)
    
    ### Spearman correlation
    cor(data.test$Gpal_vir,data.test$Gpal_avir,method = "spearman")
    
    ### Test of correlation
    ### it is possible you get a warning here; this is not an error.
    cor.test(data.test$Gpal_vir,data.test$Gpal_avir,method = "spearman")
    
    

```

##### Clustering

```{r w1d2 clustering analysis ASSIGNMENT, include = TRUE,eval=FALSE}



```

##### heatmap

```{r w1d2 heatmap, include = TRUE,eval=FALSE}



```

##### Plot of correlation

```{r w1d2 correlation plot, include = TRUE,eval=FALSE}



```
