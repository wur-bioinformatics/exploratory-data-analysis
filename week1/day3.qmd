---
title: "Your digital notebook"
author: "Mark Sterken"
date: "2026-02-04"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
editor: visual
execute:
  warning: false
  message: false
---

## Your digital notebook

### Introduction to day 3

Today we will continue with goal 1 (Built basic knowledge on using R and R studio) and goal 3 (Apply common statistical tools for inspecting and analyzing data). We will also start with goal 4 (Be able to document your code in a clear & concise way). You will find out that we've already covered this by requiring you to write a script. Now we'll add one more layer to it, the notebook.

New today, is that you get an assignment. In this assignment, you are expected to go over goal 5 \[(Apply the data-science cycle (load, inspect, ~~clean~~, analyse, present)\]. Also, this is not new, we've been doing it for the past two days.

As on the pervious days, we will be making use of a real data-case.

::: {.callout-tip appearance="simple"}
#### Plant growth impairment by nematodes.

You already know that plant susceptibility to nematodes is a complex trait, with a lot of variation in susceptibility between individuals of the same species. This variation also exists for the dose-response relation between plants and nematodes. The data you will be using today is derived from a study by Jaap-Jan Willig, who used a camera platform to track the growth of 960 *A. thaliana* plants simultaneously[@Willig2023].

![The relationship between the inoculation density (P~i~) of *H. schachtii* and green canopy area (relative yield). Nine-day-old Arabidopsis seedlings were inoculated with 20 different densities of *H. schachtii* juveniles (0--100 J2s g^--1^ of dry sand) in 200 ml pots containing 200 g of dry sand. (A) Experimental set up and colour channel decomposition of photos to extract the green leaf surface from the images. (B) Average growth curve of Arabidopsis plants inoculated with different inoculum densities of *H. schachtii* from 0 to 21 dpi. Line fitting was based on a LOESS regression. The red box indicates the data that are used for fitting the Seinhorst yield loss equation (n=10--24). (C) Fitting according to the Seinhorst yield loss equation. Parameter values for Seinhorst's equation for the relationship between initial population density (P~i~) of *H. schachtii* and measured leaf surface area. P~i~ and tolerance limit (TSYLM) are expressed in *H. schachtii* (per g of dry sand) while, the minimal yield (m) is the lowest proportion of the maximum green canopy area (cm^2^) (Ymax) at 21 dpi. The goodness of fit of the model on the data is expressed as the coefficient of determination (R^2^).](Week1_day3_Willig.jpeg)

The goal of this research was to develop a platform that allowed us to track in real-time plant growth under below-ground biotic stress caused by nematode infection. Doing this experiment, we identified a compensation response where, at low levels of infection, plants perform better than in uninfected conditions. 
:::


#### How to do code-review




#### Set up your R environment
For the basics of setting up your environment, you can check Chapter 1.



#### Quarto markdown



### Analysis of plant-growth data 




#### Inspecting data part

##### Inspecting data using functions
Last time we used `head()`, `tail()`, and `summarise()`. Again apply these functions. But now we also want to check which piece is which. For that the functions `length`, `ncol()`, `nrow()`, and `dim()` are handy.

```{r w1d2_check_the_data_ASSIGNMENT, include = TRUE,eval=FALSE}

    # Check the first 6 rows
    head(data_csv)

    # Check the last 6 rows
    tail(data_csv)
    
    # get a summary of the data
    summary(data_csv)
 
    # get the length of a vector
    length(data_csv)
    
    # get the number of columns
    ncol(data_csv)
    
    # get the number of rows
    nrow(data_csv)
    
    # get the dimensions of an object
    dim(data_csv)
    
```

##### Inspecting data using histograms
Like last time, plot a histogram using [ggplot2](https://ggplot2.tidyverse.org/).

```{r w1d2_make_a_ggplot2_histogram_ASSIGNMENT, include = TRUE,eval=FALSE}

        ggplot2::ggplot(YOUROBJECT,aes(x=Gpal_vir)) +
        geom_histogram()

        ggplot2::ggplot(YOUROBJECT,aes(x=Gpal_avir)) +
        geom_histogram()
    
```


#### Cleaning and preparing the data
Now you have the dimensions of the puzzle pieces. You can combine them in only one way. First you need to join the two pieces (1 and 2) with the equal number of rows using `cbind()`. Second, you need to add the piece (3) that now has the same number of columns using `rbind()`. Finally, you need to add the last piece (4) of data using `merge()`.

```{r w1d2_combine_pieces_ASSIGNMENT, include = TRUE,eval=FALSE}

        ### bind objects together over columns
        pheno_data <- cbind(YOURPIECE1,YOURPIECE2)
        
        ### binds objects together over rows
        pheno_data <- rbind(pheno_data,YOURPIECE3)
        
        ### merges objects, seeks for the same identitier, or you can provide it using by.x and by.y
        pheno_data <- merge(pheno_data,YOURPIECE4)
    
```

After combining, you can inspect the data again, for instance using `geom_histogram()`.


#### Analyzing
        
##### Normal distribution 
Same as before, let's figure out whether the data is normally distributed. For this we can use the functions `stat_qq()` and `stat_qq_line()`.

```{r w1d2_qqplot_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(pheno_data,aes(sample=Gpal_vir)) + 
    stat_qq() + stat_qq_line()

    ggplot(pheno_data,aes(sample=Gpal_avir)) + 
    stat_qq() + stat_qq_line()

```

Of course, you can also use a Shapiro-Wilk test (`shapiro.test`), this calculates the likelyhood whether the data follows a normal distribution.

```{r w1d2_shapiro_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    shapiro.test(pheno_data$eggmass)

```


##### Wilcoxon test
As the data is not exactly normally distributed, let's deal with that in the proper way. Now we can use a Wilcoxon Rank Sum test to compare between two samples. We first need to transform the dataset, as we do not have a single observation per row currently. For this you can use the `gather()` function from the {tidyr} package. Use functions like `head()` and `tail()` to check what happened.

```{r w1d2_gather_pallida, include = TRUE,eval=FALSE, class.source = 'fold-show'}

    data.test <- tidyr::gather(pheno_data,key="pallida",value="juveniles",-ID,-Experiment)

```

Now we can test how the potato plants performed against virulent versus avirulent _G. pallida_. Using `wilcox.test()`

```{r w1d2_Wilcox_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    wilcox.test(juveniles~pallida,data=data.test)
    ###you should have a very low p-value

```


#### Presenting
Can you now also make a plot of what you just tested? Use `geom_boxplot()` to visualize the distribution and `geom_jitter()` to show the individual measurements.

```{r w1d2_wilcoxplot_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(data.test,aes(x=pallida,y=juveniles)) +
    geom_boxplot() + geom_jitter(height=0,width=0.25)


```

There is another variable in the data as well 'Experiment'. Use `facet_grid()` to split this out. Figure out how using the [ggplot2 website](https://ggplot2.tidyverse.org/).

```{r w1d2_wilcoxplot_facet_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ggplot(data.test,aes(x=pallida,y=juveniles)) +
    geom_boxplot() + geom_jitter(height=0,width=0.25) +
    facet_grid(~Experiment)

```

Now we can add p-value as well using `annotate()`.


```{r w1d2_wilcoxplot_pval_pallida_ASSIGNMENT, include = TRUE,eval=FALSE}

    ###I'm getting the statistics here
    tmp <- tapply(data.test[,c("pallida","juveniles")],data.test$Experiment,function(x){
                                broom::tidy(wilcox.test(x$juveniles~x$pallida))})
   
    statplot <- do.call(rbind,tmp) %>%
                mutate(Experiment=1:3,pallida=1.5,juveniles=max(data.test$juveniles),
                       label=paste("p =",signif(p.value,2)))

    p1 <- ggplot(data.test,aes(x=pallida,y=juveniles)) +
          geom_boxplot() + geom_jitter(height=0,width=0.25) +
          facet_grid(~Experiment) + geom_text(aes(label=label),data=statplot)

```

##### Saving plots
Now save the plot, check back at Tutorial 1

```{r w1d2_pdf_of_pallida_plot, include = TRUE, eval=FALSE}

    pdf(file="Figure_pallida.pdf",width=3,height=4)
        print(p1)
    dev.off()   
        
```

#### Inspecting data part 2

##### Correlation analysis
As the data is not exactly normally distributed, we also have to deal with that when calculating correlation. For normal (distribution) correlation, you should use the Pearson method. This is the standard setting for `cor()`. We first need to transform the dataset, as we do not have a single observation per row currently. For this you can use the `gather()` function from the {tidyr} package. Use functions like `head()` and `tail()` to check what happened.


```{r w1d2 correlation analysis ASSIGNMENT, include = TRUE,eval=FALSE}
    
    data.test <- pheno_data

    ### Pearson correlation
    cor(data.test$Gpal_vir,data.test$Gpal_avir)
    
    ### Spearman correlation
    cor(data.test$Gpal_vir,data.test$Gpal_avir,method = "spearman")
    
    ### Test of correlation
    ### it is possible you get a warning here; this is not an error.
    cor.test(data.test$Gpal_vir,data.test$Gpal_avir,method = "spearman")
    
    

```

##### Clustering



```{r w1d2 clustering analysis ASSIGNMENT, include = TRUE,eval=FALSE}



```


##### heatmap

```{r w1d2 heatmap, include = TRUE,eval=FALSE}



```


##### Plot of correlation



```{r w1d2 correlation plot, include = TRUE,eval=FALSE}



```

















### Assignment for code-review week 1






















